{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093ee937-f65f-4ef7-b175-2fffc165b371",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michael/miniconda3/envs/mingpttutorial/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_dataset_builder, get_dataset_split_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324c33e4-d23b-44ee-a8f0-3ece5aa34489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = \"bigbio/med_qa\"\n",
    "DATASET_CONFIG = \"med_qa_en_source\"\n",
    "ds_builder = load_dataset_builder(DATASET_NAME,DATASET_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10bcd705-f928-4b5a-84b3-f7775b4067e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this work, we present the first free-form multiple-choice OpenQA dataset for solving medical problems, MedQA,\n",
      "collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and\n",
      "traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. Together\n",
      "with the question data, we also collect and release a large-scale corpus from medical textbooks from which the reading\n",
      "comprehension models can obtain necessary knowledge for answering the questions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ds_builder.info.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a06609-a5cd-40f1-a36e-0977e58c9c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset med_qa (/Users/michael/.cache/huggingface/datasets/bigbio___med_qa/med_qa_en_source/1.0.0/cfb3883fd412613f1938bbb3449a43e18bd2428b691726183a0d3c9b590f885d)\n"
     ]
    }
   ],
   "source": [
    "train_ds = load_dataset(DATASET_NAME, DATASET_CONFIG, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c891ee17-01cd-4092-bc74-d5717d114740",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta_info': 'step2&3',\n",
       " 'question': 'A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?',\n",
       " 'answer_idx': 'E',\n",
       " 'answer': 'Nitrofurantoin',\n",
       " 'options': [{'key': 'A', 'value': 'Ampicillin'},\n",
       "  {'key': 'B', 'value': 'Ceftriaxone'},\n",
       "  {'key': 'C', 'value': 'Ciprofloxacin'},\n",
       "  {'key': 'D', 'value': 'Doxycycline'},\n",
       "  {'key': 'E', 'value': 'Nitrofurantoin'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b52eb84a-2792-498a-af66-fa442fc94e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mingpt.bpe import BPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d705118-7984-4bac-864c-23976355e1c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bpe_tokenizer = BPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1dae492-65f2-4bed-a14a-a17f59e8989d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   32,  2242,    12,  1941,    12,   727, 10423,  2415,   379,  2534,\n",
       "          2745, 47110, 10969,   351,  9482,  2402,  2956,  1883,    13,  1375,\n",
       "          2585,   340,  2067,   352,  1110,  2084,   290,   468,   587, 42373,\n",
       "          3805,  7722,   517,  1660,   290,  2263, 41286,  8396,  7925,    13,\n",
       "          1375,  4306,  5300,   880,   290,   318,  3940,   416,   257,  6253,\n",
       "           329,   607, 10241,    13,  2332,  5951,   318, 10111,    13,    22,\n",
       "          7200,    37,   357,  2623,    13,    20,  7200,    34,   828,  2910,\n",
       "          3833,   318, 19409,    14,  3324,  8085,    39,    70,    11, 19445,\n",
       "           318,  4019,    14,  1084,    11, 21483,   602,   389,   678,    14,\n",
       "          1084,    11,   290, 11863, 36275,   318,  9661,     4,   319,  2119,\n",
       "          1633,    13, 16331,  2814,   318, 12411,   329,   281,  8889,   286,\n",
       "          1575,  2502,   660, 24427,  9848, 15403,  1108,   290,   257,  9067,\n",
       "           312, 41303,    13,  9022,   286,   262,  1708,   318,   262,  1266,\n",
       "          3513,   329,   428,  5827,    30]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer(train_ds[0]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b12118bd-a9ef-4735-9be5-ab153cc74bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      " 23\n",
      "-\n",
      "year\n",
      "-\n",
      "old\n",
      " pregnant\n",
      " woman\n",
      " at\n",
      " 22\n",
      " weeks\n",
      " gestation\n",
      " presents\n",
      " with\n",
      " burning\n",
      " upon\n",
      " urination\n",
      ".\n",
      " She\n",
      " states\n",
      " it\n",
      " started\n",
      " 1\n",
      " day\n",
      " ago\n",
      " and\n",
      " has\n",
      " been\n",
      " worsening\n",
      " despite\n",
      " drinking\n",
      " more\n",
      " water\n",
      " and\n",
      " taking\n",
      " cranberry\n",
      " extract\n",
      ".\n",
      " She\n",
      " otherwise\n",
      " feels\n",
      " well\n",
      " and\n",
      " is\n",
      " followed\n",
      " by\n",
      " a\n",
      " doctor\n",
      " for\n",
      " her\n",
      " pregnancy\n",
      ".\n",
      " Her\n",
      " temperature\n",
      " is\n",
      " 97\n",
      ".\n",
      "7\n",
      "°\n",
      "F\n",
      " (\n",
      "36\n",
      ".\n",
      "5\n",
      "°\n",
      "C\n",
      "),\n",
      " blood\n",
      " pressure\n",
      " is\n",
      " 122\n",
      "/\n",
      "77\n",
      " mmHg\n",
      ",\n",
      " pulse\n",
      " is\n",
      " 80\n",
      "/\n",
      "min\n",
      ",\n",
      " respirations\n",
      " are\n",
      " 19\n",
      "/\n",
      "min\n",
      ",\n",
      " and\n",
      " oxygen\n",
      " saturation\n",
      " is\n",
      " 98\n",
      "%\n",
      " on\n",
      " room\n",
      " air\n",
      ".\n",
      " Physical\n",
      " exam\n",
      " is\n",
      " notable\n",
      " for\n",
      " an\n",
      " absence\n",
      " of\n",
      " costovertebral\n",
      " angle\n",
      " tenderness\n",
      " and\n",
      " a\n",
      " gravid\n",
      " uterus\n",
      ".\n",
      " Which\n",
      " of\n",
      " the\n",
      " following\n",
      " is\n",
      " the\n",
      " best\n",
      " treatment\n",
      " for\n",
      " this\n",
      " patient\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "results = bpe_tokenizer.encoder.encode_and_show_work(train_ds[0]['question'])\n",
    "# print(train_ds[0]['question'])\n",
    "for a in results['parts']:\n",
    "    print(a['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9105b35-b65b-44ca-84e2-03005d89e021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_examples(example):\n",
    "    training_sentence = f\"{example['question']}\\nAnswer: {example['answer']}\\n\"\n",
    "    return bpe_tokenizer(training_sentence)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bdab74b-cd96-4b22-9902-03c49d951927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_examples = [encode_examples(ex) for ex in train_ds]\n",
    "\n",
    "# I only want to keep examples longer than 128 tokens\n",
    "# I only want to use the last 129 tokens of each example\n",
    "tokenized_train = [ex[-129:] for ex in tokenizer_examples if len(ex) >= 129]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "768ce216-7355-4cdc-846b-bb2105c94685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SimpleMedQADataset(Dataset):\n",
    "    def __init__(self, tokenized_examples):\n",
    "        self.tokenized_examples = tokenized_examples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_examples[idx][:-1], self.tokenized_examples[idx][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f6738c2-942c-45b1-b0c1-1e8f4a3ed677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = SimpleMedQADataset(tokenized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "933d304f-63e7-48d3-a0d3-66408672129f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.85M\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt2'\n",
    "model_config.vocab_size = 50257\n",
    "model_config.block_size = 256\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2077549-07e7-4d11-8063-b69080a0a215",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cpu\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 2000\n",
    "train_config.num_workers = 0\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "282c1305-1d8b-4788-8417-41f207f3e758",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 10.97411\n",
      "iter_dt 60800.91ms; iter 10: train loss 6.92168\n",
      "iter_dt 54592.10ms; iter 20: train loss 6.47418\n",
      "iter_dt 58699.42ms; iter 30: train loss 6.10319\n",
      "iter_dt 64744.87ms; iter 40: train loss 5.61564\n",
      "iter_dt 62520.68ms; iter 50: train loss 5.25659\n",
      "iter_dt 57952.78ms; iter 60: train loss 4.84473\n",
      "iter_dt 60909.48ms; iter 70: train loss 4.51911\n",
      "iter_dt 58607.89ms; iter 80: train loss 4.47657\n",
      "iter_dt 58216.79ms; iter 90: train loss 4.31750\n",
      "iter_dt 62382.39ms; iter 100: train loss 4.20844\n",
      "iter_dt 59524.22ms; iter 110: train loss 3.79563\n",
      "iter_dt 58650.43ms; iter 120: train loss 3.84761\n",
      "iter_dt 59997.55ms; iter 130: train loss 3.56417\n",
      "iter_dt 68485.96ms; iter 140: train loss 3.73688\n",
      "iter_dt 55112.46ms; iter 150: train loss 3.50524\n",
      "iter_dt 56766.52ms; iter 160: train loss 3.31506\n",
      "iter_dt 60999.63ms; iter 170: train loss 3.24630\n",
      "iter_dt 55462.97ms; iter 180: train loss 3.39938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter_dt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39miter_dt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mms; iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39miter_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m trainer\u001b[38;5;241m.\u001b[39mset_callback(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_batch_end\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_end_callback)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/practical-architecture-1/minGPT/mingpt/trainer.py:93\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# forward the model\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m logits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# backprop and update the parameters\u001b[39;00m\n\u001b[1;32m     96\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mingpttutorial/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/practical-architecture-1/minGPT/mingpt/model.py:273\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    271\u001b[0m     x \u001b[38;5;241m=\u001b[39m block(x)\n\u001b[1;32m    272\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[0;32m--> 273\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# if we are given some desired targets also calculate the loss\u001b[39;00m\n\u001b[1;32m    276\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mingpttutorial/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mingpttutorial/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 10 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51584155-f5fb-42e2-a05d-14f1e6755bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "inputs = bpe_tokenizer(train_ds[idx]['question']+\"\\nAnswer: \")\n",
    "outputs = model.generate(inputs, max_new_tokens=20, temperature=0.9, top_k=10, do_sample=True)\n",
    "\n",
    "try:\n",
    "    offset = list(outputs[0][-20:]).index(198)\n",
    "except:\n",
    "    offset = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dcceb9bb-e863-4ee5-be4b-858c5583bbc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'erythematous membranes; however, and decreased edema is shown\\npHg-mediated'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_ds[idx]['question'])\n",
    "bpe_tokenizer.decode(outputs[0][len(inputs[0]):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a6f7854-84fd-47f4-9424-c8089d1f53ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset, len(inputs[0])+offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8a6a1101-5fda-4d67-86ce-b14586033c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[198]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4fed7f8a-fd38-435b-9613-823481336f19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(outputs[0][-50:]).index(198)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5189b175-ab8d-475f-8ec0-d5440cffd53f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
