{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20790c78-72ea-4e96-bdc9-3a0c0e12ee86",
   "metadata": {},
   "source": [
    "# Transformer Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd82d2-b25a-458e-8954-ae709cb6d3e1",
   "metadata": {},
   "source": [
    "We are going to train a GPT model on a medical question answering dataset.  We will go definition by definition in Andrej Karpathy's minGPT implementation to understand the Transformer decoder architecture from scratch.  One we have defined the architecture, we will run a few training steps on a medium-sized dataset and then will interact with our model using a Gradio interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04cddd-f1c2-46c6-bd1d-02ac63567bf2",
   "metadata": {},
   "source": [
    "# 1. `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "635036c6-c09f-484a-9c05-2728b5d51099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference to: https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py\n",
    "\n",
    "# Code comments added for clarity noted by 'MH: '\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from mingpt.utils import CfgNode as CN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed1cde0-0621-494f-86f5-545323de4ea8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b6792-a940-4b24-b0fc-2e36451b4060",
   "metadata": {},
   "source": [
    "While not necessarily a requirement of Transformer architecture, the Gaussian Error Linear Units (GELU) activation function was used in the original BERT paper and subsequently used in GPT.  One of the key benefits of the activation function is a non-zero derivative for small negative values which could possibly mitigate the number of dead neurons that evolve during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8109f41-fb3c-4b0c-b4ce-4b5d095fc000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference to: https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L21C1-L27C107\n",
    "\n",
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12944cc-1718-4ae4-a286-a3ba06236977",
   "metadata": {},
   "source": [
    "![GELU Activation Function](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-27_at_12.48.44_PM.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bcb8c9-ee04-49b7-9c2b-ab8bb0c76c07",
   "metadata": {},
   "source": [
    "Also note that the GELU activation function is available in the torch library is more optimized than this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "197f7585-64fe-4af7-8e7c-6bd8edcbebd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quick comparison of the implementations\n",
    "library_gelu = nn.GELU()\n",
    "new_gelu = NewGELU()\n",
    "\n",
    "# Build a test domain x\n",
    "x = torch.linspace(-3.0, 3.0, 100)\n",
    "\n",
    "# Check that function outputs are \"close\" over the test domain\n",
    "assert torch.allclose(\n",
    "    library_gelu(x),\n",
    "    new_gelu(x),\n",
    "    atol=0.001,\n",
    "    rtol=0.001,\n",
    "), \"Implementations are not close, check your math, friend\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea73b5-9dcf-44e4-b3af-4cdcbe8da63e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2. Causal Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcdc5d8-e6e5-4125-a902-fcd9c003b668",
   "metadata": {},
   "source": [
    "Much of a Transformer's power comes from modeling pair-wise interactions between the elements of a sequence.\n",
    "This is our first meaty code block and the most important block to understand where causal models get their strong directional bias in sequence modeling.  Please note the only difference between this implementation of multi-headed attention and that for a Transformer encoder is the causal masking step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f36d06bb-506a-43cd-8d27-ed8e49b59a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference to: https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L29\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \"\"\"MH: \n",
    "        In our passed configuration, we provide the following hyperparatmers:\n",
    "         - n_embed - the size of the embedding dimension\n",
    "         - n_head - the number of self-attention heads\n",
    "         - attn_pdrop, resid_pdrop - dropout probabilitys for the attention activations\n",
    "                                     and residual features respectively\n",
    "         - block_size - effectively, a maximum sequence length parameter\n",
    "        \"\"\"\n",
    "        \n",
    "        # MH: This check ensures that our embedding dimension is divisible by the number of heads\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        \n",
    "        # MH: ^^^Note that this is a shortcut to avoid instantiating three different linear layers\n",
    "        # for each of the 3 projections (key, query, value)\n",
    "        \n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        \n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        \n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(\n",
    "            torch.ones(\n",
    "                config.block_size,\n",
    "                config.block_size)\n",
    "        ).view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "        \"\"\"MH: \n",
    "        Our causal mask will now be a lower triangular matrix filled with 1's in the lower triangle.\n",
    "        The mask ensures that attention weights for \"future\" elements of a sequence are zero.\n",
    "        \n",
    "        For `block_size` = 3, the matrix in the final two axes will be:\n",
    "        [ 1, 0, 0 ]\n",
    "        [ 1, 1, 0 ]\n",
    "        [ 1, 1, 1 ]\n",
    "        \n",
    "        Note that the call to `torch.tensor.view` reshapes the mask to have dimensionaity of:\n",
    "        [1, 1, block_size, block_size].  The first two stubbed dimensions allow for the matrix multplication\n",
    "        of the mask to be broadcast over the batch and sequence dimensions of the attention weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        \n",
    "        # MH: ^^^ Remember our shortcut earlier?  This step basically separates out the 3 projections\n",
    "        # from the lone Linear module we declared earlier\n",
    "        \n",
    "        \n",
    "        # MH: vvv These three steps essentially reshape our embedding dimension into the subspaces\n",
    "        # for each of heads of attention -  this is why we make sure that the embedding dimension is\n",
    "        # divisible by the number of attention heads, this step breaks if that assertion would be false   \n",
    "        # If n_head = 1, this step would be unnecessary\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        \n",
    "        # MH: ^^^ Note that we are calculating the pre-softmax attention weights across all positions\n",
    "        # scaled down by the square root of the size of the head, in the next step is where the\n",
    "        # \"causal masking\" comes into play\n",
    "        \n",
    "        \n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        # MH: ^^^ This is a somewhat subtle, but common implementation choice.\n",
    "        # We could either 1) calculate all of the attention weights, normalize with the softmax, multiply by the \n",
    "        # causal mask and then renormalize to so that the attention weights sum to 1 OR\n",
    "        # 2) we can set all of the \"future\" pre-activation attention weights to negative infinity\n",
    "        # This second implementation is much more numerically stable and efficient\n",
    "        \n",
    "        \n",
    "        att = F.softmax(att, dim=-1)\n",
    "        \n",
    "        # MH: Here is where we dropout some of the attention weights during training\n",
    "        att = self.attn_dropout(att)\n",
    "        \n",
    "        # MH: Now we calculate the attention weighted value features for each head\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        \n",
    "        # MH: Return all of the feature shapes back to the full embedding size (undo the reshape for the multiple heads)\n",
    "        # If n_head = 1, this step would not be necessary\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        # MH: This is our second normalization - dropout before sending back the to the residual\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c1820-bac6-4ab6-b0c4-65ebd78556b7",
   "metadata": {},
   "source": [
    "Let's run a quick test with some fake data to check our understanding of the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9784167a-f19b-407e-91a0-22479d49eb36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up some test paramters\n",
    "B = 3 # Assume a batch size of 3\n",
    "T = 5 # Assume that the longest sequence is 5\n",
    "C = 32 # Assume an embedding dimension of 32\n",
    "\n",
    "cm_test_cn = CN()\n",
    "\n",
    "cm_test_cn.n_embd = C\n",
    "cm_test_cn.n_head = 8\n",
    "cm_test_cn.block_size = 8 # Needs to be longer than T\n",
    "cm_test_cn.attn_pdrop, cm_test_cn.resid_pdrop = 0.1, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88837c99-83cf-4a75-904a-1140d4026af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize our test data\n",
    "X = torch.randn((B, T, C))\n",
    "\n",
    "# Initalize our self-attention module\n",
    "csa = CausalSelfAttention(cm_test_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2782424f-abc9-45e4-b2f6-25489c381822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = csa(X)\n",
    "\n",
    "assert output.shape == X.shape, \"Something went wrong, shapes must match for residuals!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c71a3cf-cb1b-4605-84fd-942e752bea15",
   "metadata": {},
   "source": [
    "### !!! Check your knowledge 1.2\n",
    "* What shape should the `bias` buffer be? How can you check if?\n",
    "* What function does the `bias` buffer perform?\n",
    "* For these parameters, what is the dimension of each head calculation? How can you check it?\n",
    "* What happens if change the number of heads to 10? Try it!\n",
    "* What happens if the block size is too short?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de3d122c-a8d4-439b-ade6-335f31cb51dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the shape of the `csa` object's `bias`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72bc790-665e-4407-9ba6-ce4ddd9d4b89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write here what dimension you think each of the heads will have\n",
    "\n",
    "\n",
    "# Rewrite the line of code from `CausalSelfAttention` that calculates this size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "203276cd-c3f2-4ac2-a385-dbfc81b5d857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the code above to instiate a new `CausalSelfAttention` with a 10 attention heads instead of 8\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29de7c52-0452-4464-83cf-075ef964203e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new set of test data (X2) where T > our configuration's `block_size` and see what happens\n",
    "# when you call `csa` with X2 as the input\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f9948d-fd48-45f4-889c-762136ec8e8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3. Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e029a8a5-f12e-4ae8-ac82-ec459c8c6223",
   "metadata": {},
   "source": [
    "Remember our diagram from our lecture before - this collection of transformations just gets repeated L times where L is the number of our layers.\n",
    "\n",
    "- Attention\n",
    "- Add Residual + Layer Norm \n",
    "- Feedforward Network\n",
    "- Add Residual + Layer Norm\n",
    "\n",
    "The exact placement of the layer normalizations is a design choice but ultimately, it used to ensure that activations do not explode across several layers of residual addition operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e50d7b22-7623-48e0-9c09-83bc0194ca77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        \n",
    "        # MH: Note the feature dimension expansion of 4 is just a magic number\n",
    "        # from the original implementation\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            act     = NewGELU(),\n",
    "            dropout = nn.Dropout(config.resid_pdrop),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        \n",
    "        \"\"\"MH: \n",
    "        We are creating a function here that will call our feedforward network modules (FFN with dropout)\n",
    "        1. First apply expansion projection to our features: [B, T, C] -> [B, T, 4xC]\n",
    "        2. Then apply our GELU activation pointwise\n",
    "        3. Then apply our contraction: [B, T, 4xC] -> [B, T, C]\n",
    "        4. Finally, apply our dropout regularization\n",
    "        \"\"\"\n",
    "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # MH: Note how the residual path is maintained all the way through\n",
    "        # and that layer normalization is applied to the features before transformation\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe6837-07af-434b-a866-f647c1306441",
   "metadata": {},
   "source": [
    "## !!! Check your knowledge 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c3382-250e-421b-90b5-f2f6c656ebfa",
   "metadata": {},
   "source": [
    "* How many layer norms are in a block?\n",
    "* What function do the layer norms serve?\n",
    "* How many residual connections are within each block?\n",
    "* How could we redefine `Block.mlpf` using PyTorch's built-in `nn.Sequential` module type?\n",
    "  * This is a type that accepts a list of modules as its contructor\n",
    "  * When the module is called, it runs each of the modules in the list one by one passing the output each time\n",
    "* How do the input dimensions compare to the output dimensions of each block?\n",
    "* What is the size of the `out_features` the `Block.mlp.c_fc` Linear module with the test config above? How can you check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8e919b8-4c15-4912-9ba9-5df51ef05dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize one block module\n",
    "one_block = Block(cm_test_cn)\n",
    "\n",
    "# Run it on our test data\n",
    "block_output = one_block(X)\n",
    "\n",
    "assert block_output.shape == X.shape, \"Something went wrong, shapes must match for residuals!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1006b45e-224c-4804-9210-aea536191b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the size of out_features\n",
    "# one_block.mlp.c_fc.out_features\n",
    "# Where did that number even come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125d52e-8a5a-4efd-8ad4-2848e6e8acc3",
   "metadata": {},
   "source": [
    "## 1.4. GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d6ab6-7a59-4a7e-a0d0-7f611f88625a",
   "metadata": {},
   "source": [
    "Now let us put this all together.  I'm going to do split up the original implementation so that I can try to add some more in-depth explanation between model definitions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db53e0-cea1-4dcc-b2b2-4eac3e3c1b6c",
   "metadata": {},
   "source": [
    "### 1.4.1 Defaults and Object Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527ee17-e6b6-4ec6-8713-89399a6aeeb2",
   "metadata": {},
   "source": [
    "In this block of code we initialize the module's submodules and set up \n",
    "some default configurations.  We can see the global architecure take shape here\n",
    "and see some of the key design decisions that set GPT apart from BERT and other Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "772cab33-764d-461b-a925-8a697c23dd94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference: https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L95C1-L161C63\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\" GPT Language Model \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CN() # CN is essentially just a minGPT data class for holding configuration settings and hyperparameters\n",
    "        \n",
    "        # either model_type or (n_layer, n_head, n_embd) must be given in the config\n",
    "        C.model_type = 'gpt'\n",
    "        C.n_layer = None\n",
    "        C.n_head = None\n",
    "        C.n_embd =  None\n",
    "        \n",
    "        # these options must be filled in externally\n",
    "        C.vocab_size = None\n",
    "        C.block_size = None\n",
    "        \n",
    "        # dropout hyperparameters\n",
    "        C.embd_pdrop = 0.1\n",
    "        C.resid_pdrop = 0.1\n",
    "        C.attn_pdrop = 0.1\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # MH: Make sure that user specifies these configuration settings\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.block_size = config.block_size\n",
    "        \n",
    "        # MH: This next block just provides some handy combinations of hyperparameters based\n",
    "        # on published versions of GPT - it is assumed that prior researchers have done a fair\n",
    "        # bit of work to fiddle with these numbers to get reasonable success with these combinations\n",
    "        type_given = config.model_type is not None\n",
    "        params_given = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n",
    "        assert type_given ^ params_given # exactly one of these (XOR)\n",
    "        if type_given:\n",
    "            # translate from model_type to detailed configuration\n",
    "            config.merge_from_dict({\n",
    "                # names follow the huggingface naming conventions\n",
    "                # GPT-1\n",
    "                'openai-gpt':   dict(n_layer=12, n_head=12, n_embd=768),  # 117M params\n",
    "                # GPT-2 configs\n",
    "                'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "                'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "                'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "                'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "                # Gophers\n",
    "                'gopher-44m':   dict(n_layer=8, n_head=16, n_embd=512),\n",
    "                # (there are a number more...)\n",
    "                # I made these tiny models up\n",
    "                'gpt-mini':     dict(n_layer=6, n_head=6, n_embd=192),\n",
    "                'gpt-micro':    dict(n_layer=4, n_head=4, n_embd=128),\n",
    "                'gpt-nano':     dict(n_layer=3, n_head=3, n_embd=48),\n",
    "            }[config.model_type])\n",
    "\n",
    "        \"\"\" MH: \n",
    "        All of that work and the model can just be summarized like this:\n",
    "        1. Converting tokens to feature vectors\n",
    "        1.a. Use a Look up table for token representations (embeddings)\n",
    "        1.b. A look up table for position embeddings - slightly perturb token representations based on \n",
    "             their position in the sequence\n",
    "        3. A stack of Transformer blocks\n",
    "        4. A final \"look up table\" for translating final feature vectors into output tokens - LM head\n",
    "            - Normally done a softmax over logits - essentialy soft reverse look up table\n",
    "            - Please note in \"Attention is All You Need\" this is the transpose of our `wte` with weight tying\n",
    "        \"\"\"\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.embd_pdrop),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" MH: This internal function initalizes all of the weights\n",
    "        This is a common convention in PyTorch to declare such an private initialization function\n",
    "        and call it in the model's constructor\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c89f84-9439-4069-b97d-b0026327d634",
   "metadata": {},
   "source": [
    "#### !!!! Check your knowledge 1.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8869f37-d387-43c4-b8ae-7dfbf89c3cad",
   "metadata": {},
   "source": [
    "* What is the dimensionality of each head in the \"gpt2-xl\" configuration above?\n",
    "* How could you modify the code above to tie the embedding and final token prediction heads together?\n",
    "  * What benefits might this change offer?\n",
    "* What does the `vocab_size` parameter do?\n",
    "* What are we initializing all of our `bias` parameters in our `Linear` modules to? How can you check?\n",
    "\n",
    "* Try to initialize a GPT model with:\n",
    "  1. The default configuration\n",
    "  2. Of a \"gpt-nano\" model type\n",
    "  3. Vocab size of 100\n",
    "  4. A block size of 200\n",
    "  \n",
    "* How many parameters with this configuration?\n",
    "* What is the maximum sequence length I can provide?\n",
    "* What is the size of the `wte` embedding with these paramaters? How can you check?\n",
    "* What is the size of the `out_features` of the `lm_head`? How can you check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56c0ba9e-8f46-4b76-b1a4-e033576282d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.10M\n"
     ]
    }
   ],
   "source": [
    "this_config = GPT.get_default_config()\n",
    "this_config.model_type = \"gpt-nano\"\n",
    "this_config.vocab_size = 100\n",
    "this_config.block_size = 200\n",
    "\n",
    "this_gpt = GPT(this_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64d8ef65-0be4-449f-9991-4cfea3697773",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100, 48)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_gpt.transformer.wte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "286a8018-f6ad-45cb-9f3f-f5fa01006564",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_gpt.lm_head.out_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b22a1b2-c83b-4067-b91a-f084cebcd519",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.4.2 `GPT.forward`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9389ebec-4daa-4d43-85d8-3b912b2047a7",
   "metadata": {},
   "source": [
    "In the `forward` method in PyTorch is where we define how the sub-modules connect together:\n",
    "* Convert input tokens to a sequence of vectors\n",
    "* Pertub the vectors by the position embeddings depending on where the token sits in a sequence\n",
    "* Pass the representations through each of the L blocks (where L is the number of layers\n",
    "* Convert the final sequence of feature representations into next token probabilities\n",
    "* \\[Training only\\] Calculate the cross-entropy between the expected next token probability and the actual token and return it as our loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce8420d9-e5f5-47f8-a563-fa2067601e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference: https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L260\n",
    "\n",
    "# Ignore the second class definition - hack for defining a class across multiple cells\n",
    "class GPT(GPT):\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35b5c20-223f-42cd-9158-e9d86305dca3",
   "metadata": {},
   "source": [
    "### !!! Check your understanding 1.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7354166e-267d-49a2-99e2-0de9ebda1709",
   "metadata": {},
   "source": [
    "* For the following questions assume the following parameters:\n",
    "    * I am using batch sizes of 10 examples per batch\n",
    "    * My data has a vocabulary size of 100\n",
    "    * My examples are at most 120 items long but I will use a block_size of 128 for extra room.\n",
    "    * I want to use a \"gpt-nano\" model\n",
    "\n",
    "\n",
    "* During my forward pass:\n",
    "    * What are the dimensions of the input?\n",
    "    * What is the expected shape of `tok_emb`?\n",
    "    * What is the expected shape of `pos_emb`?\n",
    "    * What are the dimensions of the output?\n",
    "    * What does the element located at[0, 0, 0] of the first output tensor represent?\n",
    "    \n",
    "* Check you answers with a test of the implementations\n",
    "   * Create fake input data with a call to `torch.randint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f116a73-c3c0-4e11-b7ed-80c938000739",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.10M\n"
     ]
    }
   ],
   "source": [
    "config_142 = GPT.get_default_config()\n",
    "config_142.model_type = \"gpt-nano\"\n",
    "config_142.vocab_size = 100\n",
    "config_142.block_size = 128\n",
    "\n",
    "gpt_142 = GPT(config_142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad1f7240-7232-4eb2-ae99-a426cdd4ccf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_142 = torch.randint(100,(10, 120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9020bcc2-4654-4332-9c28-8e3bfa759b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output, _ = gpt_142(X_142)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b291e20-4937-4863-93b3-dfc613ba0a4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.4.3 `GPT.generate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0673d9f-83a1-48af-874b-34d982871079",
   "metadata": {},
   "source": [
    "Given that our GPT model is primarily tasked with generating a discrete field of next token probabilities conditioned on some input text and what we really care about is the next most likely probability of tokens, we can use our `GPT.generate` function to randomly sample from this probability field, one token at a time.  We can repeat this process `max_new_tokens` number of times to extract a likely completion conditioned on the input sequence from out model.  Since we have not trained our model, this will be a gibberish list of numbers, but we will just focus on the guts of the algorithm for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11bed682-33c6-4d26-a1fc-9a057f59c6ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference: https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L282\n",
    "\n",
    "class GPT(GPT):\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # either sample from the distribution or take the most likely element\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51369e91-685c-4702-afc9-577653d44ceb",
   "metadata": {},
   "source": [
    "### !!! Check your understanding 1.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7508348-8ebb-43ed-99a2-6fe139eb6934",
   "metadata": {},
   "source": [
    "* If I initialize a `GPT` with the correct configuration, why would I expect the output to be gibberish?\n",
    "* Why minimizing the cross-entropy of the next token prediction task make the output look less like gibberish?\n",
    "* What does a higher `temperature` argument in the `GPT.generate` method do to sample outputs?\n",
    "  * In this implementation, what happens if I set `temperature` = 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a6ebd94-6b75-481e-bb51-7341e9d4e5db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.10M\n"
     ]
    }
   ],
   "source": [
    "config_143 = GPT.get_default_config()\n",
    "config_143.model_type = \"gpt-nano\"\n",
    "config_143.vocab_size = 100\n",
    "config_143.block_size = 128\n",
    "\n",
    "gpt_143 = GPT(config_143)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa159fb2-1805-43b9-837a-3e01c21226d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_143 = torch.randint(100,(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "015e84a5-84fe-40ac-8ae6-6a223f15103e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_output = gpt_143.generate(X_143, max_new_tokens=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5393bd7a-923b-4c64-a0a9-ab63deb40949",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage in:  tensor([[96, 65, 54, 91, 77]])\n",
      "Garbage out:  tensor([ 7, 91,  2, 67, 19])\n"
     ]
    }
   ],
   "source": [
    "print(\"Garbage in: \", X_143)\n",
    "print(\"Garbage out: \", sample_output[0,-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425f450e-3fb2-4af9-b037-17661c6ffb67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
