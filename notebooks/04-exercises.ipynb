{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e01aec40-615d-44e0-bd5c-3e0ddcd9468d",
   "metadata": {},
   "source": [
    "### Coding exercise #1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362e2a2f-3d1b-4689-bf28-eb3055fad043",
   "metadata": {},
   "source": [
    "### Coding exercise #2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee9894-357c-43de-903b-f15e4a5fc79d",
   "metadata": {},
   "source": [
    "### Coding exercise #3: Implementing Single-Head Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e893310f-c429-46d6-9bc4-7f4a8aa56541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "457738e7-1785-4828-ba68-ad4cf83e06b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352fa89-7735-4507-b161-3abff1743560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Initialize a single-head attention block.\n",
    "        Args:\n",
    "            embed_dim (int): The dimensionality of input embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Initialize the learnable weight matrices for queries, keys, and values\n",
    "        self.query = # Complete this .. \n",
    "        \n",
    "        # Scaling factor for dot product attention\n",
    "        self.scaling = None  # 1/sqrt(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute single-head attention for the input.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, embed_dim)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_length, embed_dim)\n",
    "        \"\"\"\n",
    "        # Get batch size, sequence length, and embedding dimension\n",
    "        batch_size, seq_length, embed_dim = x.shape\n",
    "        \n",
    "        # TODO: Compute query, key, and value matrices\n",
    "        Q = None # Complete this\n",
    "        K = None\n",
    "        V = None\n",
    "        \n",
    "        # TODO: Compute scaled dot-product attention scores\n",
    "        # Hint: You can use torch.bmm() for batched matrix multiplication \n",
    "        attention_scores = None # Complete this\n",
    "        \n",
    "        # TODO: Apply softmax to get attention weights\n",
    "        attention_weights = None  # Complete this\n",
    "        \n",
    "        # TODO: Compute final attention output\n",
    "        output = None  # Complete this\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc48d0c0-2b0a-4d98-9245-23ba7a8188c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample input tensor\n",
    "batch_size, seq_length, embed_dim = 2, 4, 8\n",
    "x = torch.randn(batch_size, seq_length, embed_dim)\n",
    "\n",
    "# Initialize the attention module\n",
    "attention = SingleHeadAttention(embed_dim)\n",
    "\n",
    "# Compute attention\n",
    "output = attention(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6504f528-0790-4139-ba9e-84dbc63476ba",
   "metadata": {},
   "source": [
    "### Code-exercise #4.1 Implementing positional encodings on input embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e9a7b9b-8482-4456-86a3-46b1dbf07e58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Implement fixed sinusoidal positional encodings\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=5000):\n",
    "        \"\"\"\n",
    "        Initialize the positional encoding.\n",
    "        \n",
    "        Args:\n",
    "            embed_size (int): Size of the embeddings\n",
    "            max_len (int): Maximum sequence length to pre-compute\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # TODO: Create a matrix of shape (max_len, embed_size)\n",
    "        pe = None\n",
    "        \n",
    "        # TODO: Create position vector\n",
    "        position = None \n",
    "        \n",
    "        # Create division term for different dimensions\n",
    "        # Hint: Use the following div term\n",
    "        div_term = 1 / (10000.0 ** (torch.arange(0, embed_size, 2).float() / embed_size))\n",
    "        \n",
    "        # TODO: Fill the pe matrix with sin values for even indices and cos for odd indices\n",
    "        # pe[:, 0::2] = ??? \n",
    "        # pe[:, 1::2] = ??? \n",
    "        \n",
    "        # Register pe as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to input tensor.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, embed_size)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Input combined with positional encoding\n",
    "        \"\"\"\n",
    "        # TODO: Get sequence length from input tensor\n",
    "        seq_length = None\n",
    "        \n",
    "        # TODO: Add position encoding to input\n",
    "        x = None \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01681b52-02ad-4224-8f64-48a4cd4c6653",
   "metadata": {},
   "source": [
    "### Code-exercise #4.2 Implementing fixed positional encodings on input embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77e965-b56e-4050-8ee8-dc2e97314e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement learnable positional encodings\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=5000):\n",
    "        \"\"\"\n",
    "        Initialize the learnable positional encoding matrix.\n",
    "        \n",
    "        Args:\n",
    "            embed_size (int): Dimension of each embedding vector.\n",
    "            max_len (int): Maximum length of the sequence for positional encoding.\n",
    "        \"\"\"\n",
    "        super(LearnedPositionalEncoding, self).__init__()\n",
    "        \n",
    "        # TODO: Define a learnable positional encoding matrix with shape (max_len, embed_size)\n",
    "        # Hint: Use nn.Parameter to make it a trainable parameter\n",
    "        self.positional_encoding = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Adds the learnable positional encoding to the input tensor.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, embed_size).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Input tensor with added learnable positional encoding.\n",
    "        \"\"\"\n",
    "        # TODO: Get the sequence length from the input tensor\n",
    "        seq_length = None\n",
    "        \n",
    "        # TODO: Add positional encoding\n",
    "        x = None \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5f690d-b8bf-40c2-b0bc-600e66f5022d",
   "metadata": {},
   "source": [
    "##### Visualizing positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a69ddf72-239e-4bca-a3af-63bafcca2029",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage with visualizations\n",
    "# Test parameters\n",
    "batch_size = 32\n",
    "seq_length = 20\n",
    "embed_size = 512\n",
    "\n",
    "# Create random input tensor\n",
    "x = torch.randn(batch_size, seq_length, embed_size)\n",
    "\n",
    "# Create positional encoding instance\n",
    "pos_encoder = PositionalEncoding(embed_size)\n",
    "\n",
    "# Apply positional encoding\n",
    "output = pos_encoder(x)\n",
    "\n",
    "# Print shapes\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Visualize the positional encodings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the first 100 positions and first 20 dimensions\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(pos_encoder.pe[0, :100, :20])\n",
    "plt.xlabel('Embedding Dimension')\n",
    "plt.ylabel('Sequence Position')\n",
    "plt.title('Positional Encodings')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Plot specific dimensions\n",
    "plt.figure(figsize=(15, 5))\n",
    "dims_to_plot = [0, 1, 4, 5]  # Plot first few even/odd pairs\n",
    "for dim in dims_to_plot:\n",
    "    plt.plot(\n",
    "        pos_encoder.pe[0, :40, dim].numpy(),\n",
    "        label=f'dim {dim} ({\"even\" if dim % 2 == 0 else \"odd\"})'\n",
    "    )\n",
    "plt.legend()\n",
    "plt.title('Positional Encoding Values')\n",
    "plt.xlabel('Sequence Position')\n",
    "plt.ylabel('Encoding Value')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a14b78-351b-41c4-a9e4-f6da01f2ac96",
   "metadata": {},
   "source": [
    "### Code review #5.1: Patch embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232e171-ecee-419b-b6ef-54dc1fd75df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://github.com/FrancescoSaverioZuppichini/ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae97a8-f866-4dae-923e-d478ea2d61fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768):\n",
    "        # the size of each patch, and the embedding size for each patch.\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define a sequential layer for projecting patches. \n",
    "        self.projection = nn.Sequential(\n",
    "            # Conv2d layer for projecting image patches into embeddings:\n",
    "            # - in_channels is the number of channels in the input image.\n",
    "            # - emb_size represents the desired output dimensions for each patch (embedding size).\n",
    "            # - kernel_size and stride are set to patch_size, so it \"slides\" across the image \n",
    "            #   and outputs patches of the specified size.\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            \n",
    "            # Rearranges the output tensor:\n",
    "            # - Changes the tensor from [batch, embedding_dim, h, w] to [batch, num_patches, embedding_dim].\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "                \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Passes the input tensor through the projection layer to obtain patch embeddings.\n",
    "        x = self.projection(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09775b44-0df5-4c5c-a15a-1dad1d711fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Conv2d layer with kernel_size = stride = patch_size slides across the input image in patch x patch dimensions,\n",
    "# just as traditional patch extraction would do.\n",
    "# Each patch is processed without overlapping its neighbors as kernel_size = stride\n",
    "\n",
    "# The output channels are set to emb_size, so it outputs an embedding vector per patch directly, essentially combining the patch extraction and linear transformation into one step. \n",
    "# This reduces computation time and adds efficiency by avoiding separate operations for patching and embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef098fd9-ffaf-410d-97c9-27957ff8227b",
   "metadata": {},
   "source": [
    "### Code review #5.2: ViT architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ca154-82c3-4c04-9eed-1965fedbf0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 3,\n",
    "                patch_size: int = 16,\n",
    "                emb_size: int = 768,\n",
    "                img_size: int = 224,\n",
    "                depth: int = 12,\n",
    "                n_classes: int = 1000,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3eda20-08d5-4503-b1cd-b3ae303140e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 768, n_classes: int = 1000):\n",
    "        super().__init__(\n",
    "            # Select the [CLS] token (assuming it's at index 0)\n",
    "            Lambda(lambda x: x[:, 0]),  # Only use the [CLS] token for classification\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc35f8ec-bafb-47a9-8054-4f1cebcc0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b67334c-7373-488a-a279-baa58dbfb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 768,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b160808-1dfd-4907-9936-78da1ee39ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
