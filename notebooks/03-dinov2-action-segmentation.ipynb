{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd78c81-3329-4076-bbfd-1e2f2f114c5b",
   "metadata": {},
   "source": [
    "# 03 - Unsupervised Action Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06acb6d2-5934-49fb-a881-11fa1dd6ae2f",
   "metadata": {},
   "source": [
    "In this exercise, we are going to use features from a pretrained Vision Transformer model to perform action segmentation without labels.  The features we extract will be used to perform k-means clustering.  In theory, the clusters should represent distinct actions happening in the video.  Ideally, frames that are close to one another in time will have the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8a557-cf55-4a16-be8e-7632dd63d89e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mediapy as media\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_spatial_kmeans as tsk\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40776e-2319-42c8-9fe9-3f2cb6a3489b",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee54528-6db7-4bc2-a688-4d209dcb6bd8",
   "metadata": {},
   "source": [
    "To extract the frame-level features, we are going to use a pretrained Vision Transformer.  Specifically, we will use the Dino v2 model released by Meta AI < [Paper page](https://dinov2.metademolab.com/) >.  This instance of the Vision Transformer was pretrained without supervision and relies soley on self-supervision techniques like self-distillation (iBOT) and in experiments is seen to work very well as a feature encoder for k-NN classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3470301-dcfb-4de0-8ced-86b6ca16f51f",
   "metadata": {},
   "source": [
    "Furthermore, we will make use of the Huggingface library for its ease of use in working with pretrained models.  You can find more about the available sizes of DinoV2 models available on Huggingface Hub here: [Huggingface: DinoV2 Collection](https://huggingface.co/collections/facebook/dinov2-6526c98554b3d2576e071ce3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022fdbda-edfb-4304-be1b-547c4df96eda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# API reference here: https://huggingface.co/docs/transformers/v4.34.1/model_doc/dinov2\n",
    "\n",
    "# This function will take an image as array of RGB and convert it to a sequence of image tokens\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "\n",
    "# This function encapsulates DinoV2 model as a pretrained encoder\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "\n",
    "# Make use of our GPU by move the model weights to the GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7479f8bc-d9a1-409c-8e9a-5550949f656b",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399a4b78-b6b4-4ec1-9168-e19e14309932",
   "metadata": {},
   "source": [
    "Let's take a look at the video that we will be segmenting temporally using the [mediapy library](https://github.com/google/mediapy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8425efe-9c93-4560-aca1-679c07c7f1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXAMPLE_FILE = os.getenv('EXAMPLE_FILE')\n",
    "\n",
    "video2 = media.read_video(EXAMPLE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f1c5d-4f4a-4c87-85e8-cf8e46fcc172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "media.show_video(video2[3000:3500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad04375-6961-4b89-bef4-18cd94f96f75",
   "metadata": {},
   "source": [
    "In this video, we see:\n",
    "* A title screen\n",
    "* A doctor enter the room\n",
    "* A doctor begins speaking with the patient, asking questions\n",
    "* A doctor examines a patient's chest and listens to their lungs\n",
    "* Examines their ankles\n",
    "* Examines their legs and feet\n",
    "* A doctor closes by summarizing their assessment and plan for the patient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38c1c4b-c44c-4b71-83f1-02a395325d47",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b763ce-4068-40c7-96bd-c39bd6eb3dc9",
   "metadata": {},
   "source": [
    "In this section, we will use our DinoV2 model to extract frame level features for a sampling of frames.  At the end we will stack the list of frame level outputs into a \\[N, D\\] matrix where `N` is the number of frames and `D` is the embedding dimension of the DinoV2 encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a94b3f-9090-409d-bb12-d1598a908e67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize our list for accumulating our embeddings\n",
    "embeds = []\n",
    "\n",
    "# We want to grab every 10th frame; video is originally 30fps so we will grab 3 embeddings per second\n",
    "sampling = 10\n",
    "\n",
    "with media.VideoReader('test.mp4') as vid:\n",
    "    # For each frame...\n",
    "    for i, frame in enumerate(tqdm.tqdm(vid)):\n",
    "        # Skip unless its every 10th frame\n",
    "        if i % sampling > 0:\n",
    "            continue\n",
    "            \n",
    "        # Convert frame to PIL format\n",
    "        image = Image.fromarray(frame)\n",
    "        \n",
    "        # Use processor to extract DinoV2 patches from the image\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate the ouputs\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Only keep the pooled output - by default this is the [CLS] token\n",
    "        embeds.append(outputs.pooler_output.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1197290d-7eb8-4155-a03e-b0853c08c060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stack up the embeddings into a feature matrix\n",
    "embeds = torch.concat(embeds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b5251-6c42-426f-a21f-f097e50fc0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f7619f-8604-462e-8174-7e66f7758efc",
   "metadata": {},
   "source": [
    "## Frame Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b1b84-5f9b-4d53-b7ac-051b4bc121cc",
   "metadata": {},
   "source": [
    "Using these frame level features, we will now use k-means clustering to automatically discovering groups of similar frames.  Rather than using traditional k-means clustering we will want to take advantage of the bias that frames that are close to each other temporally most likely participate in the same action.  We will use this [spatial k-means clustering library](https://github.com/mike-holcomb/torch-spatial-kmeans) that leverages a kernel that calculates the distance as the weighted sum of the L2 distance  between embeddings as well as the L2 distance between their coordinates.  In this application, we will use the time coordinate in our to encourage adjacent frames to land in the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e774fcb-e556-4368-8f81-4fe4fa7fa133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(tsk.spatial_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8d9db-bd2e-460c-9600-ebbf82ee43a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's add a column to capture the temporal coordinate\n",
    "\n",
    "data = torch.cat([embeds, torch.linspace(-1,1,len(embeds)).unsqueeze(1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c06428-3d86-4a57-93fb-6c61d8744739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's z-normalize the data since k-means prefers hyperspherical data\n",
    "\n",
    "data = (data - data.mean(axis=1, keepdim=True))/data.std(axis=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd493e-d054-4d3d-a49c-a6d22b11d8ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Out data for processing should now be 768 feature dimensions + 1 coordinate dimension for time\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3fa57-2009-4793-8b8c-b6d706b81fed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's set our hyperparameters here that we can tune later\n",
    "n_clusters = 10\n",
    "spatial_weight = 200\n",
    "\n",
    "# Run our k_means\n",
    "centroids, cluster_assignments = tsk.spatial_kmeans(data, k=n_clusters, spatial_weight=spatial_weight, num_spatial_dims=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84280b02-d722-4670-9ce8-563f9b18b14b",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7262cbe1-ea78-464a-b888-05a5078a2442",
   "metadata": {},
   "source": [
    "Now that we have gotten our clusters, let's see on our timeline how these clusters evolve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd4035c-0b49-4db5-a195-177cfe93b93a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "action_ids = cluster_assignments.numpy()\n",
    "n_frames = len(action_ids)\n",
    "frame_numbers = np.arange(n_frames)\n",
    "\n",
    "# Define the unique actions and assign a color to each\n",
    "unique_actions = np.unique(action_ids)\n",
    "colors = plt.cm.jet(np.linspace(0, 1, len(unique_actions)))  # Using the jet colormap\n",
    "action_colors = dict(zip(unique_actions, colors))\n",
    "\n",
    "# Plotting the segmentation timeline\n",
    "plt.figure(figsize=(8, 2))  # Adjust the size as needed\n",
    "current_action = action_ids[0]\n",
    "start_frame = 0\n",
    "\n",
    "for i in range(1, n_frames):\n",
    "    if action_ids[i] != current_action:\n",
    "        plt.axvspan(start_frame, i, color=action_colors[current_action], alpha=0.9)\n",
    "        start_frame = i\n",
    "        current_action = action_ids[i]\n",
    "\n",
    "# For the last segment\n",
    "plt.axvspan(start_frame, n_frames, color=action_colors[current_action], alpha=0.9)\n",
    "\n",
    "# Add legend\n",
    "handles = [plt.Rectangle((0,0),1,1, color=action_colors[id_]) for id_ in unique_actions]\n",
    "plt.legend(handles, unique_actions, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Frame Number')\n",
    "plt.yticks([])\n",
    "plt.title('Action Segmentation Timeline')\n",
    "plt.xlim(0,n_frames)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecd6aab-129f-4e4a-b80d-4be120a67b32",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Representative Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e351334-96f8-4fca-8236-db236435de15",
   "metadata": {},
   "source": [
    "To help us understand what is in each of these clusters, let's find in the data the frames that are closest to each of the centroids of our clusters as representative frames for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdcad89-48bf-4fd7-b6af-bd200408898c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(tsk.custom_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3592913a-3124-4338-9039-dd91eeaf2d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the distance for every frame to every centroid\n",
    "dists =tsk.custom_kernel(data, centroids, spatial_weight=spatial_weight, num_spatial_dims=1)\n",
    "\n",
    "# Representative frames are the \"closest\" frames to the centroids\n",
    "representative_frames = torch.argmin(dists,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137984c3-17fd-41af-9b79-86eab7243b89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's take a look at what is in each of these frames\n",
    "\n",
    "fig, axs = plt.subplots(n_clusters, 1, gridspec_kw={\"hspace\": 0.5}, figsize=(3,15))\n",
    "\n",
    "for i, idx in enumerate(representative_frames):\n",
    "    axs[i].imshow(video2[idx*sampling])\n",
    "    axs[i].axis(\"off\")\n",
    "    axs[i].set_title(f\"cluster {i} - frame: {idx.item()*sampling}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37514e22-f66a-4cb2-85d1-fcfa99e3daf6",
   "metadata": {},
   "source": [
    "## BONUS: Multi-Modal Vision Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171efb4-0068-4c30-8a77-e1d3f44286df",
   "metadata": {},
   "source": [
    "Vision Transformers also play a role in multi-modal transformers that is deep learning architectures that combine more than one modality such as vision, textual, or audio features.  Here we use a multi-modal ViT called Blip from salesforce to automatically caption our representative frames.  The details of this architecture are beyond the scope of this course, but more details about this particular model can be found here: [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e4807-723b-43a8-9f7b-0b90a41b21de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab410e-a134-45df-9e8b-37565698bbb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(n_clusters, 1, gridspec_kw={\"hspace\": 0.5}, figsize=(3,15))\n",
    "\n",
    "# To help make the captions more grounded in our video, we can provide some conditioning text for the captions\n",
    "text = \"In an exam room,\"\n",
    "\n",
    "for i, idx in enumerate(torch.argmin(dists,axis=0)):\n",
    "    # Grab the image data for the representative frame\n",
    "    raw_image = video2[idx*sampling]\n",
    "    \n",
    "    # Convert the image and conditioning text to input tokens for the blip model\n",
    "    inputs = blip_processor(raw_image, text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Run inference to grab the joint representation of the conditioning text and representative frame\n",
    "    out = blip_model.generate(**inputs)\n",
    "    \n",
    "    # Decode the most likely tokens based on this encoding\n",
    "    caption = blip_processor.decode(out[0], skip_special_tokens=True)\n",
    "    \n",
    "    axs[i].imshow(raw_image)\n",
    "    axs[i].axis(\"off\")\n",
    "    axs[i].set_title(f\"cluster {i}: {caption}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
